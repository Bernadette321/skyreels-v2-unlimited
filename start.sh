#!/bin/bash

echo "ğŸš€ å¯åŠ¨ SkyReels V2 Docker API æœåŠ¡å™¨..."

# æ£€æµ‹GPUå’Œè®¾ç½®ä¼˜åŒ–å‚æ•°
echo "ğŸ” æ£€æµ‹GPUé…ç½®..."
if command -v nvidia-smi &> /dev/null; then
    GPU_INFO=$(nvidia-smi --query-gpu=name,memory.total --format=csv,noheader,nounits | head -n1)
    GPU_NAME=$(echo $GPU_INFO | cut -d',' -f1 | xargs)
    GPU_MEMORY=$(echo $GPU_INFO | cut -d',' -f2 | xargs)
    
    echo "ğŸ® æ£€æµ‹åˆ°GPU: $GPU_NAME ($GPU_MEMORY MB VRAM)"
    
    # æ ¹æ®GPUé…ç½®ä¼˜åŒ–å‚æ•°
    if [[ "$GPU_NAME" == *"RTX 4090"* ]]; then
        echo "âš¡ åº”ç”¨RTX 4090ä¼˜åŒ–é…ç½®..."
        export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
        export TORCH_CUDA_ARCH_LIST="8.6"
        export CUDA_LAUNCH_BLOCKING=1
        
        # RTX 4090ç‰¹å®šä¼˜åŒ–
        export OMP_NUM_THREADS=4
        export MKL_NUM_THREADS=4
        export NUMBA_CACHE_DIR=/tmp/numba_cache
        
    elif [[ "$GPU_NAME" == *"A100"* ]]; then
        echo "ğŸ”¥ åº”ç”¨A100ä¼˜åŒ–é…ç½®..."
        export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:1024
        export TORCH_CUDA_ARCH_LIST="8.0"
        export OMP_NUM_THREADS=8
        
    else
        echo "ğŸ”§ åº”ç”¨é€šç”¨GPUä¼˜åŒ–é…ç½®..."
        export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:256
        export OMP_NUM_THREADS=2
    fi
    
    # å†…å­˜é™åˆ¶æ£€æŸ¥
    if [ "$GPU_MEMORY" -lt 20000 ]; then
        echo "âš ï¸  è­¦å‘Š: VRAMå°‘äº20GBï¼Œå»ºè®®é™ä½åˆ†è¾¨ç‡åˆ°720Pä»¥ä¸‹"
        export SKYREELS_MAX_RESOLUTION="720p"
        export SKYREELS_MAX_DURATION="300"  # 5åˆ†é’Ÿ
    elif [ "$GPU_MEMORY" -lt 24000 ]; then
        echo "âœ… VRAMé€‚ä¸­ï¼Œæ”¯æŒ720Pé•¿è§†é¢‘"
        export SKYREELS_MAX_RESOLUTION="720p"
        export SKYREELS_MAX_DURATION="720"  # 12åˆ†é’Ÿ
    else
        echo "ğŸš€ é«˜VRAMé…ç½®ï¼Œæ”¯æŒ1080Pè§†é¢‘"
        export SKYREELS_MAX_RESOLUTION="1080p"
        export SKYREELS_MAX_DURATION="720"  # 12åˆ†é’Ÿ
    fi
    
else
    echo "âŒ æœªæ£€æµ‹åˆ°NVIDIA GPUï¼Œè¯·æ£€æŸ¥é©±åŠ¨å®‰è£…"
    exit 1
fi

# é€šç”¨ç¯å¢ƒå˜é‡è®¾ç½®
export CUDA_VISIBLE_DEVICES=0
export PYTHONPATH=/app/SkyReels-V2:$PYTHONPATH
export HF_HOME=/app/cache
export TRANSFORMERS_CACHE=/app/cache
export TORCH_HOME=/app/cache

# ä¼˜åŒ–è®¾ç½®
export TOKENIZERS_PARALLELISM=false
export PYTHONUNBUFFERED=1

# åˆ›å»ºå¿…è¦çš„ç›®å½•
mkdir -p /app/outputs
mkdir -p /app/cache
mkdir -p /tmp/numba_cache

# è®¾ç½®æƒé™
chmod 755 /app/outputs
chmod 755 /app/cache

echo "ğŸ“ å·¥ä½œç›®å½•: $(pwd)"
echo "ğŸ Pythonç‰ˆæœ¬: $(python --version)"
echo "ğŸ”¥ PyTorchç‰ˆæœ¬: $(python -c 'import torch; print(torch.__version__)')"

# æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
if [ ! -d "/app/SkyReels-V2" ]; then
    echo "âŒ SkyReels-V2 ä»£ç ç›®å½•ä¸å­˜åœ¨"
    exit 1
fi

# å¯åŠ¨å¥åº·ç›‘æ§ï¼ˆåå°ï¼‰
echo "ğŸ“Š å¯åŠ¨GPUç›‘æ§..."
{
    while true; do
        timestamp=$(date '+%Y-%m-%d %H:%M:%S')
        gpu_stats=$(nvidia-smi --query-gpu=utilization.gpu,memory.used,memory.total,temperature.gpu --format=csv,noheader,nounits 2>/dev/null || echo "N/A,N/A,N/A,N/A")
        echo "[$timestamp] GPU Stats: $gpu_stats" >> /app/outputs/gpu_monitor.log
        
        # æ£€æŸ¥VRAMä½¿ç”¨ç‡
        if command -v nvidia-smi &> /dev/null; then
            vram_used=$(nvidia-smi --query-gpu=memory.used --format=csv,noheader,nounits | head -n1)
            vram_total=$(nvidia-smi --query-gpu=memory.total --format=csv,noheader,nounits | head -n1)
            if [ ! -z "$vram_used" ] && [ ! -z "$vram_total" ]; then
                vram_usage=$((vram_used * 100 / vram_total))
                if [ $vram_usage -gt 90 ]; then
                    echo "[$timestamp] âš ï¸  HIGH VRAM USAGE: ${vram_usage}%" >> /app/outputs/gpu_monitor.log
                fi
            fi
        fi
        
        sleep 30
    done
} &

# å¯åŠ¨APIæœåŠ¡å™¨
echo "ğŸŒ å¯åŠ¨SkyReels V2 APIæœåŠ¡å™¨..."
echo "ğŸ”— æœåŠ¡å°†åœ¨ç«¯å£8000ä¸Šè¿è¡Œ"
echo "ğŸ“‹ APIæ–‡æ¡£åœ°å€: http://localhost:8000/docs"

cd /app

# å¯åŠ¨ä¸»æœåŠ¡
exec gunicorn \
    --bind 0.0.0.0:8000 \
    --workers 1 \
    --worker-class uvicorn.workers.UvicornWorker \
    --timeout 7200 \
    --keep-alive 30 \
    --max-requests 50 \
    --max-requests-jitter 10 \
    --preload \
    --log-level info \
    --access-logfile - \
    --error-logfile - \
    api_server:app 