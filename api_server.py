#!/usr/bin/env python3
"""
SkyReels V2 Flask API Server for 720P Long Video Generation
æ”¯æŒæ— é™é•¿åº¦è§†é¢‘ç”Ÿæˆçš„APIæœåŠ¡å™¨
"""

import os
import sys
import json
import time
import uuid
import threading
from datetime import datetime
from flask import Flask, request, jsonify, send_file
from flask_cors import CORS
import logging
import gc
import asyncio
from typing import Dict, List, Optional, Any
from pathlib import Path

import torch
import psutil
from fastapi import FastAPI, HTTPException, BackgroundTasks, UploadFile, File
from fastapi.responses import FileResponse, JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
import uvicorn

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# æ·»åŠ SkyReels V2åˆ°Pythonè·¯å¾„
sys.path.insert(0, '/app/SkyReels-V2')

try:
    from skyreels.inference import SkyReelsInference
    from skyreels.config import load_config
except ImportError as e:
    logger.error(f"Failed to import SkyReels V2: {e}")
    sys.exit(1)

app = Flask(__name__)
CORS(app)

# å…¨å±€å˜é‡
inference_engine = None
generation_status = {}  # å­˜å‚¨ç”Ÿæˆä»»åŠ¡çŠ¶æ€

# å†…å­˜ä¼˜åŒ–å™¨
class MemoryOptimizer:
    @staticmethod
    def clear_cache():
        """æ¸…ç†GPUå’Œç³»ç»Ÿç¼“å­˜"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
        gc.collect()
        logger.info("Memory cache cleared")
    
    @staticmethod
    def get_gpu_memory_info():
        """è·å–GPUå†…å­˜ä¿¡æ¯"""
        if torch.cuda.is_available():
            return {
                "allocated": torch.cuda.memory_allocated() / 1024**3,
                "cached": torch.cuda.memory_reserved() / 1024**3,
                "total": torch.cuda.get_device_properties(0).total_memory / 1024**3
            }
        return {"allocated": 0, "cached": 0, "total": 0}
    
    @staticmethod
    def optimize_model_loading():
        """ä¼˜åŒ–æ¨¡å‹åŠ è½½è®¾ç½®"""
        if torch.cuda.is_available():
            torch.backends.cudnn.benchmark = True
            torch.backends.cuda.matmul.allow_tf32 = True
            logger.info("GPU optimization settings applied")

# GPUé…ç½®æ£€æµ‹å™¨
class GPUDetector:
    def __init__(self):
        self.gpu_info = self._detect_gpu()
        self.max_resolution = os.getenv("SKYREELS_MAX_RESOLUTION", "720p")
        self.max_duration = int(os.getenv("SKYREELS_MAX_DURATION", "300"))
    
    def _detect_gpu(self):
        """æ£€æµ‹GPUé…ç½®"""
        if not torch.cuda.is_available():
            return {"name": "CPU", "memory": 0, "capability": "low"}
        
        try:
            import subprocess
            result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader,nounits'], 
                                 capture_output=True, text=True)
            if result.returncode == 0:
                lines = result.stdout.strip().split('\n')
                gpu_name, memory_mb = lines[0].split(', ')
                memory_gb = int(memory_mb) / 1024
                
                # ç¡®å®šGPUèƒ½åŠ›ç­‰çº§
                if "RTX 4090" in gpu_name or memory_gb >= 24:
                    capability = "high"
                elif "RTX 40" in gpu_name or memory_gb >= 16:
                    capability = "medium"
                else:
                    capability = "low"
                
                return {
                    "name": gpu_name.strip(),
                    "memory": memory_gb,
                    "capability": capability
                }
        except Exception as e:
            logger.warning(f"GPU detection failed: {e}")
        
        # å›é€€åˆ°PyTorchä¿¡æ¯
        props = torch.cuda.get_device_properties(0)
        return {
            "name": props.name,
            "memory": props.total_memory / 1024**3,
            "capability": "medium"
        }
    
    def validate_request(self, resolution: str, duration: int) -> Dict[str, Any]:
        """éªŒè¯è¯·æ±‚å‚æ•°æ˜¯å¦åœ¨ç¡¬ä»¶é™åˆ¶å†…"""
        warnings = []
        
        # åˆ†è¾¨ç‡æ£€æŸ¥
        resolution_limits = {
            "low": ["480p", "540p"],
            "medium": ["480p", "540p", "720p"],
            "high": ["480p", "540p", "720p", "1080p"]
        }
        
        allowed_resolutions = resolution_limits.get(self.gpu_info["capability"], ["480p"])
        if resolution not in allowed_resolutions:
            return {
                "valid": False,
                "error": f"åˆ†è¾¨ç‡ {resolution} ä¸è¢«å½“å‰GPUæ”¯æŒã€‚å»ºè®®ä½¿ç”¨: {', '.join(allowed_resolutions)}",
                "suggested_resolution": allowed_resolutions[-1]
            }
        
        # æ—¶é•¿æ£€æŸ¥
        duration_limits = {
            "low": 120,      # 2åˆ†é’Ÿ
            "medium": 300,   # 5åˆ†é’Ÿ  
            "high": 720      # 12åˆ†é’Ÿ
        }
        
        max_duration = duration_limits.get(self.gpu_info["capability"], 60)
        if duration > max_duration:
            warnings.append(f"æ—¶é•¿ {duration}s è¶…è¿‡å»ºè®®å€¼ {max_duration}sï¼Œå¯èƒ½å¯¼è‡´å†…å­˜ä¸è¶³")
        
        # ç»„åˆè­¦å‘Š
        if resolution == "1080p" and duration > 300:
            warnings.append("1080Pé•¿è§†é¢‘éœ€è¦å¤§é‡VRAMï¼Œå»ºè®®å…ˆå°è¯•720P")
        
        return {
            "valid": True,
            "warnings": warnings,
            "gpu_info": self.gpu_info
        }

# åˆå§‹åŒ–æ£€æµ‹å™¨å’Œä¼˜åŒ–å™¨
gpu_detector = GPUDetector()
memory_optimizer = MemoryOptimizer()

# è¯·æ±‚å’Œå“åº”æ¨¡å‹
class VideoGenerationRequest(BaseModel):
    prompt: str = Field(..., description="è§†é¢‘ç”Ÿæˆæç¤ºè¯")
    resolution: str = Field(default="720p", description="è§†é¢‘åˆ†è¾¨ç‡ (480p, 540p, 720p, 1080p)")
    duration: int = Field(default=300, description="è§†é¢‘æ—¶é•¿ï¼ˆç§’ï¼‰")
    guidance_scale: float = Field(default=7.5, description="å¼•å¯¼æ¯”ä¾‹")
    num_inference_steps: int = Field(default=20, description="æ¨ç†æ­¥æ•°")
    fps: int = Field(default=24, description="å¸§ç‡")
    seed: Optional[int] = Field(default=None, description="éšæœºç§å­")

class TaskStatus(BaseModel):
    task_id: str
    status: str  # "queued", "processing", "completed", "failed"
    progress: float = 0.0
    created_at: datetime
    updated_at: datetime
    result_path: Optional[str] = None
    error: Optional[str] = None
    gpu_stats: Optional[Dict] = None

# å…¨å±€çŠ¶æ€ç®¡ç†
task_queue: Dict[str, TaskStatus] = {}
current_task: Optional[str] = None

# FastAPIåº”ç”¨åˆå§‹åŒ–
app = FastAPI(
    title="SkyReels V2 API",
    description="äººå·¥æ™ºèƒ½è§†é¢‘ç”ŸæˆæœåŠ¡ - é’ˆå¯¹RTX 4090ä¼˜åŒ–",
    version="2.0.0"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# åº”ç”¨å¯åŠ¨äº‹ä»¶
@app.on_event("startup")
async def startup_event():
    """åº”ç”¨å¯åŠ¨æ—¶çš„åˆå§‹åŒ–"""
    logger.info("ğŸš€ SkyReels V2 API æœåŠ¡å™¨å¯åŠ¨ä¸­...")
    logger.info(f"ğŸ® æ£€æµ‹åˆ°GPU: {gpu_detector.gpu_info['name']} ({gpu_detector.gpu_info['memory']:.1f}GB)")
    logger.info(f"ğŸ“Š æœ€å¤§åˆ†è¾¨ç‡: {gpu_detector.max_resolution}, æœ€å¤§æ—¶é•¿: {gpu_detector.max_duration}s")
    
    # åº”ç”¨å†…å­˜ä¼˜åŒ–
    memory_optimizer.optimize_model_loading()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path("/app/outputs")
    output_dir.mkdir(exist_ok=True)
    
    logger.info("âœ… SkyReels V2 API æœåŠ¡å™¨å¯åŠ¨å®Œæˆ")

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    memory_info = memory_optimizer.get_gpu_memory_info()
    system_memory = psutil.virtual_memory()
    
    return {
        "status": "healthy",
        "service": "SkyReels V2 API",
        "version": "2.0.0",
        "gpu_info": gpu_detector.gpu_info,
        "memory": {
            "gpu": memory_info,
            "system": {
                "total": system_memory.total / 1024**3,
                "available": system_memory.available / 1024**3,
                "usage_percent": system_memory.percent
            }
        },
        "active_tasks": len([t for t in task_queue.values() if t.status in ["queued", "processing"]]),
        "current_task": current_task
    }

@app.get("/models/info")
async def get_model_info():
    """è·å–æ¨¡å‹ä¿¡æ¯"""
    return {
        "model_name": "SkyReels V2",
        "model_type": "Video Generation",
        "supported_resolutions": ["480p", "540p", "720p", "1080p"],
        "max_resolution": gpu_detector.max_resolution,
        "max_duration": gpu_detector.max_duration,
        "recommended_settings": {
            gpu_detector.gpu_info["capability"]: {
                "resolution": "720p" if gpu_detector.gpu_info["capability"] in ["medium", "high"] else "540p",
                "max_duration": 300 if gpu_detector.gpu_info["capability"] == "high" else 120,
                "guidance_scale": 7.5,
                "steps": 20
            }
        }
    }

@app.post("/generate")
async def generate_video(request: VideoGenerationRequest, background_tasks: BackgroundTasks):
    """å¯åŠ¨è§†é¢‘ç”Ÿæˆä»»åŠ¡"""
    # éªŒè¯è¯·æ±‚å‚æ•°
    validation = gpu_detector.validate_request(request.resolution, request.duration)
    if not validation["valid"]:
        raise HTTPException(status_code=400, detail=validation["error"])
    
    # ç”Ÿæˆä»»åŠ¡ID
    task_id = str(uuid.uuid4())
    
    # åˆ›å»ºä»»åŠ¡çŠ¶æ€
    task_status = TaskStatus(
        task_id=task_id,
        status="queued",
        created_at=datetime.now(),
        updated_at=datetime.now()
    )
    
    task_queue[task_id] = task_status
    
    # æ·»åŠ åå°ä»»åŠ¡
    background_tasks.add_task(process_video_generation, task_id, request)
    
    response = {
        "task_id": task_id,
        "status": "queued",
        "message": "è§†é¢‘ç”Ÿæˆä»»åŠ¡å·²æ’é˜Ÿ"
    }
    
    # æ·»åŠ è­¦å‘Šä¿¡æ¯
    if validation.get("warnings"):
        response["warnings"] = validation["warnings"]
    
    return response

async def process_video_generation(task_id: str, request: VideoGenerationRequest):
    """å¤„ç†è§†é¢‘ç”Ÿæˆçš„åå°ä»»åŠ¡"""
    global current_task
    
    try:
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€
        task_queue[task_id].status = "processing"
        task_queue[task_id].updated_at = datetime.now()
        current_task = task_id
        
        logger.info(f"ğŸ“¹ å¼€å§‹ç”Ÿæˆè§†é¢‘ (ä»»åŠ¡ID: {task_id})")
        logger.info(f"ğŸ“‹ å‚æ•°: {request.resolution}, {request.duration}s, {request.prompt[:50]}...")
        
        # æ¸…ç†å†…å­˜
        memory_optimizer.clear_cache()
        
        # æ¨¡æ‹Ÿè§†é¢‘ç”Ÿæˆè¿‡ç¨‹ (è¿™é‡Œéœ€è¦æ›¿æ¢ä¸ºå®é™…çš„SkyReels V2ç”Ÿæˆä»£ç )
        for progress in range(0, 101, 10):
            await asyncio.sleep(1)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
            task_queue[task_id].progress = progress / 100.0
            task_queue[task_id].updated_at = datetime.now()
            task_queue[task_id].gpu_stats = memory_optimizer.get_gpu_memory_info()
            
            logger.info(f"ğŸ“ˆ ä»»åŠ¡ {task_id} è¿›åº¦: {progress}%")
        
        # æ¨¡æ‹Ÿè¾“å‡ºæ–‡ä»¶
        output_path = f"/app/outputs/video_{task_id}.mp4"
        
        # è¿™é‡Œåº”è¯¥æ˜¯å®é™…çš„æ–‡ä»¶ç”Ÿæˆ
        Path(output_path).touch()  # åˆ›å»ºå ä½æ–‡ä»¶
        
        # å®Œæˆä»»åŠ¡
        task_queue[task_id].status = "completed"
        task_queue[task_id].result_path = output_path
        task_queue[task_id].progress = 1.0
        task_queue[task_id].updated_at = datetime.now()
        current_task = None
        
        logger.info(f"âœ… è§†é¢‘ç”Ÿæˆå®Œæˆ (ä»»åŠ¡ID: {task_id})")
        
    except Exception as e:
        logger.error(f"âŒ è§†é¢‘ç”Ÿæˆå¤±è´¥ (ä»»åŠ¡ID: {task_id}): {str(e)}")
        task_queue[task_id].status = "failed"
        task_queue[task_id].error = str(e)
        task_queue[task_id].updated_at = datetime.now()
        current_task = None
    finally:
        # æ¸…ç†å†…å­˜
        memory_optimizer.clear_cache()

@app.get("/tasks/{task_id}/status")
async def get_task_status(task_id: str):
    """è·å–ä»»åŠ¡çŠ¶æ€"""
    if task_id not in task_queue:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡æœªæ‰¾åˆ°")
    
    return task_queue[task_id]

@app.get("/tasks")
async def get_all_tasks():
    """è·å–æ‰€æœ‰ä»»åŠ¡åˆ—è¡¨"""
    return {
        "tasks": list(task_queue.values()),
        "total": len(task_queue),
        "active": len([t for t in task_queue.values() if t.status in ["queued", "processing"]]),
        "completed": len([t for t in task_queue.values() if t.status == "completed"]),
        "failed": len([t for t in task_queue.values() if t.status == "failed"])
    }

@app.get("/tasks/{task_id}/download")
async def download_video(task_id: str):
    """ä¸‹è½½ç”Ÿæˆçš„è§†é¢‘"""
    if task_id not in task_queue:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡æœªæ‰¾åˆ°")
    
    task = task_queue[task_id]
    if task.status != "completed":
        raise HTTPException(status_code=400, detail="è§†é¢‘è¿˜æœªç”Ÿæˆå®Œæˆ")
    
    if not task.result_path or not Path(task.result_path).exists():
        raise HTTPException(status_code=404, detail="è§†é¢‘æ–‡ä»¶æœªæ‰¾åˆ°")
    
    return FileResponse(
        path=task.result_path,
        filename=f"skyreels_video_{task_id}.mp4",
        media_type="video/mp4"
    )

@app.delete("/tasks/{task_id}")
async def delete_task(task_id: str):
    """åˆ é™¤ä»»åŠ¡å’Œç›¸å…³æ–‡ä»¶"""
    if task_id not in task_queue:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡æœªæ‰¾åˆ°")
    
    task = task_queue[task_id]
    
    # åˆ é™¤ç»“æœæ–‡ä»¶
    if task.result_path and Path(task.result_path).exists():
        Path(task.result_path).unlink()
    
    # åˆ é™¤ä»»åŠ¡è®°å½•
    del task_queue[task_id]
    
    return {"message": f"ä»»åŠ¡ {task_id} å·²åˆ é™¤"}

@app.post("/system/cleanup")
async def cleanup_system():
    """æ¸…ç†ç³»ç»Ÿç¼“å­˜å’Œä¸´æ—¶æ–‡ä»¶"""
    memory_optimizer.clear_cache()
    
    # æ¸…ç†æ—§çš„ä»»åŠ¡è®°å½• (ä¿ç•™æœ€è¿‘100ä¸ª)
    if len(task_queue) > 100:
        # æŒ‰æ—¶é—´æ’åºï¼Œåˆ é™¤æœ€æ—§çš„ä»»åŠ¡
        sorted_tasks = sorted(task_queue.items(), key=lambda x: x[1].created_at)
        for task_id, _ in sorted_tasks[:-100]:
            if task_id in task_queue:
                del task_queue[task_id]
    
    return {
        "message": "ç³»ç»Ÿæ¸…ç†å®Œæˆ",
        "memory_after_cleanup": memory_optimizer.get_gpu_memory_info()
    }

if __name__ == "__main__":
    uvicorn.run(
        "api_server:app",
        host="0.0.0.0",
        port=8000,
        workers=1,
        log_level="info"
    ) 