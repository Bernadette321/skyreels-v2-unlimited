#!/usr/bin/env python3
"""
SkyReels V2 Unlimited API Server
æ”¯æŒæ— é™åˆ¶é•¿è§†é¢‘ç”Ÿæˆçš„APIæœåŠ¡å™¨ - æ— æ—¶é•¿å’Œåˆ†è¾¨ç‡é™åˆ¶
"""

import os
import sys
import gc
import logging
import asyncio
import uuid
from typing import Dict, List, Optional, Any
from datetime import datetime, timedelta
from pathlib import Path

import torch
import psutil
from fastapi import FastAPI, HTTPException, BackgroundTasks, UploadFile, File
from fastapi.responses import FileResponse, JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
import uvicorn

# å¯ç”¨æ— é™åˆ¶æ¨¡å¼
os.environ["SKYREELS_UNLIMITED_MODE"] = "true"

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# æ·»åŠ SkyReels V2åˆ°Pythonè·¯å¾„
sys.path.insert(0, '/app/SkyReels-V2')

# å†…å­˜ä¼˜åŒ–å™¨
class MemoryOptimizer:
    @staticmethod
    def clear_cache():
        """æ¸…ç†GPUå’Œç³»ç»Ÿç¼“å­˜"""
        if torch.cuda.is_available():
            for i in range(torch.cuda.device_count()):
                torch.cuda.set_device(i)
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
        gc.collect()
        logger.info("Memory cache cleared")
    
    @staticmethod
    def get_gpu_memory_info():
        """è·å–æ‰€æœ‰GPUå†…å­˜ä¿¡æ¯"""
        if not torch.cuda.is_available():
            return {"total_gpus": 0, "gpus": []}
        
        gpus = []
        for i in range(torch.cuda.device_count()):
            torch.cuda.set_device(i)
            gpus.append({
                "gpu_id": i,
                "name": torch.cuda.get_device_name(i),
                "allocated": torch.cuda.memory_allocated(i) / 1024**3,
                "cached": torch.cuda.memory_reserved(i) / 1024**3,
                "total": torch.cuda.get_device_properties(i).total_memory / 1024**3
            })
        
        return {
            "total_gpus": len(gpus),
            "gpus": gpus,
            "total_memory": sum(gpu["total"] for gpu in gpus)
        }
    
    @staticmethod
    def optimize_model_loading():
        """ä¼˜åŒ–æ¨¡å‹åŠ è½½è®¾ç½®"""
        if torch.cuda.is_available():
            torch.backends.cudnn.benchmark = True
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
            logger.info("GPU optimization settings applied")

# æ— é™åˆ¶GPUæ£€æµ‹å™¨
class UnlimitedGPUDetector:
    def __init__(self):
        self.gpu_info = self._detect_gpu()
        # æ— é™åˆ¶è®¾ç½®
        self.max_resolution = os.getenv("SKYREELS_MAX_RESOLUTION", "1080p")
        self.max_duration = int(os.getenv("SKYREELS_MAX_DURATION", "7200"))  # 2å°æ—¶
        self.enable_4k = os.getenv("SKYREELS_ENABLE_4K", "true").lower() == "true"
        self.mode = os.getenv("SKYREELS_MODE", "unlimited")
    
    def _detect_gpu(self):
        """æ£€æµ‹GPUé…ç½®"""
        if not torch.cuda.is_available():
            return {"name": "CPU", "memory": 0, "capability": "none", "gpu_count": 0}
        
        try:
            import subprocess
            result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader,nounits'], 
                                 capture_output=True, text=True)
            if result.returncode == 0:
                lines = result.stdout.strip().split('\n')
                gpu_count = len(lines)
                total_memory = 0
                gpu_names = []
                
                for line in lines:
                    gpu_name, memory_mb = line.split(', ')
                    gpu_names.append(gpu_name.strip())
                    total_memory += int(memory_mb)
                
                total_memory_gb = total_memory / 1024
                
                # æ ¹æ®æ€»VRAMç¡®å®šèƒ½åŠ›ç­‰çº§
                if total_memory_gb >= 70:
                    capability = "unlimited"
                elif total_memory_gb >= 40:
                    capability = "extended"
                elif total_memory_gb >= 20:
                    capability = "standard"
                else:
                    capability = "limited"
                
                return {
                    "name": ", ".join(gpu_names),
                    "memory": total_memory_gb,
                    "capability": capability,
                    "gpu_count": gpu_count,
                    "individual_gpus": gpu_names
                }
        except Exception as e:
            logger.warning(f"GPU detection failed: {e}")
        
        # å›é€€åˆ°PyTorchä¿¡æ¯
        gpu_count = torch.cuda.device_count()
        total_memory = sum(torch.cuda.get_device_properties(i).total_memory for i in range(gpu_count)) / 1024**3
        
        return {
            "name": f"{gpu_count}x GPU",
            "memory": total_memory,
            "capability": "standard",
            "gpu_count": gpu_count
        }
    
    def validate_request(self, resolution: str, duration: int) -> Dict[str, Any]:
        """æ— é™åˆ¶éªŒè¯ - ä»…æä¾›å»ºè®®å’Œä¼°ç®—ï¼Œä¸é˜»æ­¢ä»»ä½•è¯·æ±‚"""
        warnings = []
        
        # æ—¶é•¿å»ºè®®
        if duration > 3600:  # è¶…è¿‡1å°æ—¶
            hours = duration // 3600
            warnings.append(f"ç”Ÿæˆ{hours}å°æ—¶è§†é¢‘éœ€è¦å¾ˆé•¿æ—¶é—´ï¼Œå»ºè®®åˆç†å®‰æ’")
        elif duration > 1800:  # è¶…è¿‡30åˆ†é’Ÿ
            minutes = duration // 60
            warnings.append(f"ç”Ÿæˆ{minutes}åˆ†é’Ÿè§†é¢‘éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…")
        
        # åˆ†è¾¨ç‡å»ºè®®
        if resolution == "4k":
            if duration > 600:  # 4Kè¶…è¿‡10åˆ†é’Ÿ
                warnings.append("4Ké•¿è§†é¢‘éœ€è¦æå¤§è®¡ç®—èµ„æºï¼Œç”Ÿæˆæ—¶é—´ä¼šæ˜¾è‘—å»¶é•¿")
            if self.gpu_info["memory"] < 40:
                warnings.append("4Kè§†é¢‘å»ºè®®ä½¿ç”¨40GB+æ˜¾å­˜çš„GPUä»¥è·å¾—æœ€ä½³æ€§èƒ½")
        elif resolution == "1080p":
            if duration > 1800 and self.gpu_info["memory"] < 24:
                warnings.append("1080Pé•¿è§†é¢‘å»ºè®®ä½¿ç”¨24GB+æ˜¾å­˜çš„GPU")
        
        # GPUèƒ½åŠ›å»ºè®®
        if self.gpu_info["capability"] == "limited":
            warnings.append("å½“å‰GPUé…ç½®å¯èƒ½å½±å“ç”Ÿæˆé€Ÿåº¦ï¼Œå»ºè®®é™ä½åˆ†è¾¨ç‡æˆ–ç¼©çŸ­æ—¶é•¿")
        
        return {
            "valid": True,  # å§‹ç»ˆå…è®¸
            "warnings": warnings,
            "estimated_time": self._estimate_time(resolution, duration),
            "gpu_info": self.gpu_info,
            "recommended_settings": self._get_recommended_settings(resolution, duration)
        }
    
    def _estimate_time(self, resolution: str, duration: int) -> int:
        """ä¼°ç®—ç”Ÿæˆæ—¶é—´ï¼ˆåˆ†é’Ÿï¼‰"""
        base_time = duration / 60  # åŸºç¡€æ—¶é—´æ¯”ä¾‹
        
        # åˆ†è¾¨ç‡ç³»æ•°
        if resolution == "4k":
            resolution_multiplier = 8
        elif resolution == "1080p":
            resolution_multiplier = 3
        elif resolution == "720p":
            resolution_multiplier = 1.5
        else:
            resolution_multiplier = 1
        
        # GPUèƒ½åŠ›ç³»æ•°
        capability_multipliers = {
            "unlimited": 0.5,    # H100/å¤šA100
            "extended": 0.8,     # A100 80GB
            "standard": 1.2,     # A100 40GB/RTX 4090
            "limited": 2.0       # è¾ƒä½é…ç½®
        }
        
        capability_multiplier = capability_multipliers.get(self.gpu_info["capability"], 1.0)
        
        estimated_minutes = base_time * resolution_multiplier * capability_multiplier
        return max(int(estimated_minutes), 1)  # è‡³å°‘1åˆ†é’Ÿ
    
    def _get_recommended_settings(self, resolution: str, duration: int) -> Dict[str, Any]:
        """è·å–æ¨èè®¾ç½®"""
        settings = {
            "guidance_scale": 7.5,
            "num_inference_steps": 50,
            "fps": 30
        }
        
        # æ ¹æ®GPUèƒ½åŠ›è°ƒæ•´è®¾ç½®
        if self.gpu_info["capability"] == "unlimited":
            settings.update({
                "num_inference_steps": 100,  # æœ€é«˜è´¨é‡
                "enable_upscaling": True,
                "enable_audio": True
            })
        elif self.gpu_info["capability"] == "extended":
            settings.update({
                "num_inference_steps": 75,
                "enable_audio": True
            })
        elif self.gpu_info["capability"] == "standard":
            settings.update({
                "num_inference_steps": 50
            })
        else:  # limited
            settings.update({
                "num_inference_steps": 30,
                "guidance_scale": 6.0
            })
        
        return settings

# åˆå§‹åŒ–æ£€æµ‹å™¨å’Œä¼˜åŒ–å™¨
gpu_detector = UnlimitedGPUDetector()
memory_optimizer = MemoryOptimizer()

# æ— é™åˆ¶è§†é¢‘ç”Ÿæˆè¯·æ±‚æ¨¡å‹
class UnlimitedVideoRequest(BaseModel):
    prompt: str = Field(..., description="è§†é¢‘ç”Ÿæˆæç¤ºè¯", min_length=1, max_length=2000)
    resolution: str = Field(default="1080p", description="åˆ†è¾¨ç‡: 480p, 540p, 720p, 1080p, 4k")
    duration: int = Field(default=720, description="è§†é¢‘æ—¶é•¿ï¼ˆç§’ï¼‰ï¼Œæ— ä¸Šé™", ge=1, le=7200)
    quality: str = Field(default="high", description="è´¨é‡ç­‰çº§: standard, high, ultra")
    fps: int = Field(default=30, description="å¸§ç‡: 24, 30, 60")
    guidance_scale: float = Field(default=7.5, description="å¼•å¯¼æ¯”ä¾‹", ge=1.0, le=20.0)
    num_inference_steps: int = Field(default=50, description="æ¨ç†æ­¥æ•°ï¼Œæ›´å¤š=æ›´é«˜è´¨é‡", ge=10, le=200)
    seed: Optional[int] = Field(default=None, description="éšæœºç§å­")
    enable_audio: bool = Field(default=True, description="å¯ç”¨éŸ³é¢‘ç”Ÿæˆ")
    enable_upscaling: bool = Field(default=False, description="å¯ç”¨AIè¶…åˆ†è¾¨ç‡")
    batch_size: int = Field(default=1, description="æ‰¹å¤„ç†å¤§å°", ge=1, le=4)

class TaskStatus(BaseModel):
    task_id: str
    status: str  # "queued", "processing", "completed", "failed", "cancelled"
    progress: float = 0.0
    created_at: datetime
    updated_at: datetime
    estimated_completion: Optional[datetime] = None
    result_path: Optional[str] = None
    error: Optional[str] = None
    warnings: List[str] = []
    gpu_stats: Optional[Dict] = None
    generation_params: Optional[Dict] = None

# å…¨å±€çŠ¶æ€ç®¡ç†
task_queue: Dict[str, TaskStatus] = {}
current_tasks: List[str] = []  # æ”¯æŒå¹¶å‘ä»»åŠ¡

# FastAPIåº”ç”¨åˆå§‹åŒ–
app = FastAPI(
    title="SkyReels V2 Unlimited API",
    description="æ— é™åˆ¶AIè§†é¢‘ç”ŸæˆæœåŠ¡ - æ”¯æŒä»»æ„æ—¶é•¿å’Œåˆ†è¾¨ç‡",
    version="2.0-unlimited",
    docs_url="/docs",
    redoc_url="/redoc"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# åº”ç”¨å¯åŠ¨äº‹ä»¶
@app.on_event("startup")
async def startup_event():
    """åº”ç”¨å¯åŠ¨æ—¶çš„åˆå§‹åŒ–"""
    logger.info("ğŸ”¥ SkyReels V2 Unlimited API æœåŠ¡å™¨å¯åŠ¨ä¸­...")
    logger.info(f"ğŸ® GPUé…ç½®: {gpu_detector.gpu_info['name']}")
    logger.info(f"ğŸ’¾ æ€»VRAM: {gpu_detector.gpu_info['memory']:.1f}GB")
    logger.info(f"ğŸ”¢ GPUæ•°é‡: {gpu_detector.gpu_info['gpu_count']}")
    logger.info(f"âš¡ èƒ½åŠ›ç­‰çº§: {gpu_detector.gpu_info['capability']}")
    logger.info(f"ğŸ“Š æœ€å¤§åˆ†è¾¨ç‡: {gpu_detector.max_resolution}")
    logger.info(f"â±ï¸  æœ€å¤§æ—¶é•¿: {gpu_detector.max_duration}s ({gpu_detector.max_duration//60}åˆ†é’Ÿ)")
    
    # åº”ç”¨å†…å­˜ä¼˜åŒ–
    memory_optimizer.optimize_model_loading()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    for dir_name in ["videos", "temp", "audio", "logs"]:
        dir_path = Path(f"/app/outputs/{dir_name}")
        dir_path.mkdir(parents=True, exist_ok=True)
    
    logger.info("âœ… SkyReels V2 Unlimited API æœåŠ¡å™¨å¯åŠ¨å®Œæˆ")

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    memory_info = memory_optimizer.get_gpu_memory_info()
    system_memory = psutil.virtual_memory()
    
    return {
        "status": "healthy",
        "service": "SkyReels V2 Unlimited API",
        "version": "2.0-unlimited",
        "mode": gpu_detector.mode,
        "gpu_info": gpu_detector.gpu_info,
        "memory": {
            "gpu": memory_info,
            "system": {
                "total": system_memory.total / 1024**3,
                "available": system_memory.available / 1024**3,
                "usage_percent": system_memory.percent
            }
        },
        "tasks": {
            "total": len(task_queue),
            "active": len(current_tasks),
            "queued": len([t for t in task_queue.values() if t.status == "queued"]),
            "processing": len([t for t in task_queue.values() if t.status == "processing"]),
            "completed": len([t for t in task_queue.values() if t.status == "completed"]),
            "failed": len([t for t in task_queue.values() if t.status == "failed"])
        },
        "capabilities": {
            "max_resolution": gpu_detector.max_resolution,
            "max_duration": gpu_detector.max_duration,
            "enable_4k": gpu_detector.enable_4k,
            "multi_gpu": gpu_detector.gpu_info["gpu_count"] > 1,
            "unlimited_mode": True
        }
    }

@app.get("/models/info")
async def get_model_info():
    """è·å–æ¨¡å‹ä¿¡æ¯"""
    return {
        "model_name": "SkyReels V2 Unlimited",
        "model_type": "Video Generation",
        "supported_resolutions": ["480p", "540p", "720p", "1080p", "4k"] if gpu_detector.enable_4k else ["480p", "540p", "720p", "1080p"],
        "max_resolution": gpu_detector.max_resolution,
        "max_duration": gpu_detector.max_duration,
        "features": [
            "æ— é™åˆ¶è§†é¢‘æ—¶é•¿",
            "æ”¯æŒ4Kåˆ†è¾¨ç‡" if gpu_detector.enable_4k else "æ”¯æŒ1080Påˆ†è¾¨ç‡",
            "AIéŸ³é¢‘ç”Ÿæˆ",
            "æ™ºèƒ½è¶…åˆ†è¾¨ç‡",
            "å¤šGPUå¹¶è¡Œå¤„ç†" if gpu_detector.gpu_info["gpu_count"] > 1 else "å•GPUå¤„ç†",
            "å®æ—¶è¿›åº¦ç›‘æ§",
            "æ‰¹é‡å¤„ç†æ”¯æŒ"
        ],
        "recommended_settings": gpu_detector._get_recommended_settings("1080p", 720)
    }

@app.post("/generate")
async def generate_video(request: UnlimitedVideoRequest, background_tasks: BackgroundTasks):
    """å¯åŠ¨æ— é™åˆ¶è§†é¢‘ç”Ÿæˆä»»åŠ¡"""
    # éªŒè¯è¯·æ±‚å‚æ•°ï¼ˆä»…è·å–å»ºè®®ï¼Œä¸é˜»æ­¢ï¼‰
    validation = gpu_detector.validate_request(request.resolution, request.duration)
    
    # ç”Ÿæˆä»»åŠ¡ID
    task_id = str(uuid.uuid4())
    
    # ä¼°ç®—å®Œæˆæ—¶é—´
    estimated_duration = validation["estimated_time"]
    estimated_completion = datetime.now() + timedelta(minutes=estimated_duration)
    
    # åˆ›å»ºä»»åŠ¡çŠ¶æ€
    task_status = TaskStatus(
        task_id=task_id,
        status="queued",
        created_at=datetime.now(),
        updated_at=datetime.now(),
        estimated_completion=estimated_completion,
        warnings=validation.get("warnings", []),
        generation_params={
            "prompt": request.prompt,
            "resolution": request.resolution,
            "duration": request.duration,
            "quality": request.quality,
            "fps": request.fps,
            "guidance_scale": request.guidance_scale,
            "num_inference_steps": request.num_inference_steps,
            "seed": request.seed,
            "enable_audio": request.enable_audio,
            "enable_upscaling": request.enable_upscaling,
            "batch_size": request.batch_size
        }
    )
    
    task_queue[task_id] = task_status
    
    # æ·»åŠ åå°ä»»åŠ¡
    background_tasks.add_task(process_unlimited_video_generation, task_id, request)
    
    response = {
        "task_id": task_id,
        "status": "queued",
        "message": f"æ— é™åˆ¶è§†é¢‘ç”Ÿæˆä»»åŠ¡å·²æ’é˜Ÿ - {request.resolution} {request.duration//60}åˆ†é’Ÿ{request.duration%60}ç§’",
        "estimated_completion": estimated_completion.isoformat(),
        "estimated_duration_minutes": estimated_duration,
        "queue_position": len([t for t in task_queue.values() if t.status == "queued"])
    }
    
    # æ·»åŠ è­¦å‘Šä¿¡æ¯
    if validation.get("warnings"):
        response["warnings"] = validation["warnings"]
    
    # æ·»åŠ æ¨èè®¾ç½®
    response["recommended_settings"] = validation["recommended_settings"]
    
    return response

async def process_unlimited_video_generation(task_id: str, request: UnlimitedVideoRequest):
    """å¤„ç†æ— é™åˆ¶è§†é¢‘ç”Ÿæˆçš„åå°ä»»åŠ¡"""
    try:
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€
        task_queue[task_id].status = "processing"
        task_queue[task_id].updated_at = datetime.now()
        current_tasks.append(task_id)
        
        logger.info(f"ğŸ¬ å¼€å§‹æ— é™åˆ¶è§†é¢‘ç”Ÿæˆ (ä»»åŠ¡ID: {task_id})")
        logger.info(f"ğŸ“‹ å‚æ•°: {request.resolution}, {request.duration}s, è´¨é‡: {request.quality}")
        logger.info(f"ğŸ¯ æç¤ºè¯: {request.prompt[:100]}...")
        
        # æ¸…ç†å†…å­˜
        memory_optimizer.clear_cache()
        
        # æ¨¡æ‹Ÿæ— é™åˆ¶è§†é¢‘ç”Ÿæˆè¿‡ç¨‹
        total_steps = 100
        for step in range(total_steps + 1):
            await asyncio.sleep(0.5)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
            
            progress = step / total_steps
            task_queue[task_id].progress = progress
            task_queue[task_id].updated_at = datetime.now()
            task_queue[task_id].gpu_stats = memory_optimizer.get_gpu_memory_info()
            
            # æ¨¡æ‹Ÿä¸åŒé˜¶æ®µ
            if step < 20:
                stage = "åˆå§‹åŒ–æ¨¡å‹å’Œå‚æ•°"
            elif step < 40:
                stage = "ç”Ÿæˆè§†é¢‘å…³é”®å¸§"
            elif step < 70:
                stage = "æ’å€¼å’Œç»†èŠ‚ä¼˜åŒ–"
            elif step < 90:
                stage = "éŸ³é¢‘ç”Ÿæˆå’ŒåŒæ­¥" if request.enable_audio else "æœ€ç»ˆæ¸²æŸ“"
            else:
                stage = "åå¤„ç†å’Œè¾“å‡º"
            
            if step % 10 == 0:
                logger.info(f"ğŸ“ˆ ä»»åŠ¡ {task_id} è¿›åº¦: {step}% - {stage}")
        
        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶è·¯å¾„
        timestamp = int(datetime.now().timestamp())
        filename = f"skyreels_unlimited_{task_id}_{timestamp}_{request.resolution}_{request.duration}s.mp4"
        output_path = f"/app/outputs/videos/{filename}"
        
        # æ¨¡æ‹Ÿæ–‡ä»¶ç”Ÿæˆ
        Path(output_path).touch()  # åˆ›å»ºå ä½æ–‡ä»¶
        
        # å¦‚æœå¯ç”¨éŸ³é¢‘ï¼Œåˆ›å»ºéŸ³é¢‘æ–‡ä»¶
        if request.enable_audio:
            audio_path = f"/app/outputs/audio/audio_{task_id}.wav"
            Path(audio_path).touch()
        
        # å®Œæˆä»»åŠ¡
        task_queue[task_id].status = "completed"
        task_queue[task_id].result_path = output_path
        task_queue[task_id].progress = 1.0
        task_queue[task_id].updated_at = datetime.now()
        
        logger.info(f"âœ… æ— é™åˆ¶è§†é¢‘ç”Ÿæˆå®Œæˆ (ä»»åŠ¡ID: {task_id})")
        logger.info(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_path}")
        
    except Exception as e:
        logger.error(f"âŒ æ— é™åˆ¶è§†é¢‘ç”Ÿæˆå¤±è´¥ (ä»»åŠ¡ID: {task_id}): {str(e)}")
        task_queue[task_id].status = "failed"
        task_queue[task_id].error = str(e)
        task_queue[task_id].updated_at = datetime.now()
    finally:
        # ä»å½“å‰ä»»åŠ¡åˆ—è¡¨ç§»é™¤
        if task_id in current_tasks:
            current_tasks.remove(task_id)
        # æ¸…ç†å†…å­˜
        memory_optimizer.clear_cache()

@app.get("/tasks/{task_id}/status")
async def get_task_status(task_id: str):
    """è·å–ä»»åŠ¡çŠ¶æ€"""
    if task_id not in task_queue:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡æœªæ‰¾åˆ°")
    
    return task_queue[task_id]

@app.get("/tasks")
async def get_all_tasks(limit: int = 50, status: Optional[str] = None):
    """è·å–ä»»åŠ¡åˆ—è¡¨"""
    tasks = list(task_queue.values())
    
    # çŠ¶æ€è¿‡æ»¤
    if status:
        tasks = [t for t in tasks if t.status == status]
    
    # æŒ‰åˆ›å»ºæ—¶é—´æ’åº
    tasks.sort(key=lambda x: x.created_at, reverse=True)
    
    # é™åˆ¶æ•°é‡
    tasks = tasks[:limit]
    
    return {
        "tasks": tasks,
        "total": len(task_queue),
        "filtered": len(tasks),
        "statistics": {
            "queued": len([t for t in task_queue.values() if t.status == "queued"]),
            "processing": len([t for t in task_queue.values() if t.status == "processing"]),
            "completed": len([t for t in task_queue.values() if t.status == "completed"]),
            "failed": len([t for t in task_queue.values() if t.status == "failed"]),
            "cancelled": len([t for t in task_queue.values() if t.status == "cancelled"])
        }
    }

@app.get("/tasks/{task_id}/download")
async def download_video(task_id: str):
    """ä¸‹è½½ç”Ÿæˆçš„è§†é¢‘"""
    if task_id not in task_queue:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡æœªæ‰¾åˆ°")
    
    task = task_queue[task_id]
    if task.status != "completed":
        raise HTTPException(status_code=400, detail=f"è§†é¢‘ç”Ÿæˆæœªå®Œæˆï¼Œå½“å‰çŠ¶æ€: {task.status}")
    
    if not task.result_path or not Path(task.result_path).exists():
        raise HTTPException(status_code=404, detail="è§†é¢‘æ–‡ä»¶æœªæ‰¾åˆ°")
    
    return FileResponse(
        path=task.result_path,
        filename=Path(task.result_path).name,
        media_type="video/mp4"
    )

@app.delete("/tasks/{task_id}")
async def delete_task(task_id: str):
    """åˆ é™¤ä»»åŠ¡å’Œç›¸å…³æ–‡ä»¶"""
    if task_id not in task_queue:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡æœªæ‰¾åˆ°")
    
    task = task_queue[task_id]
    
    # å¦‚æœä»»åŠ¡æ­£åœ¨å¤„ç†ï¼Œæ ‡è®°ä¸ºå–æ¶ˆ
    if task.status == "processing":
        task.status = "cancelled"
        task.updated_at = datetime.now()
        return {"message": f"ä»»åŠ¡ {task_id} å·²æ ‡è®°ä¸ºå–æ¶ˆ"}
    
    # åˆ é™¤ç»“æœæ–‡ä»¶
    if task.result_path and Path(task.result_path).exists():
        Path(task.result_path).unlink()
    
    # åˆ é™¤ä»»åŠ¡è®°å½•
    del task_queue[task_id]
    
    return {"message": f"ä»»åŠ¡ {task_id} å·²åˆ é™¤"}

@app.post("/system/cleanup")
async def cleanup_system():
    """æ¸…ç†ç³»ç»Ÿç¼“å­˜å’Œä¸´æ—¶æ–‡ä»¶"""
    memory_optimizer.clear_cache()
    
    # æ¸…ç†æ—§çš„ä»»åŠ¡è®°å½• (ä¿ç•™æœ€è¿‘200ä¸ª)
    if len(task_queue) > 200:
        # æŒ‰æ—¶é—´æ’åºï¼Œåˆ é™¤æœ€æ—§çš„ä»»åŠ¡
        sorted_tasks = sorted(task_queue.items(), key=lambda x: x[1].created_at)
        for task_id, task in sorted_tasks[:-200]:
            # åˆ é™¤ç›¸å…³æ–‡ä»¶
            if task.result_path and Path(task.result_path).exists():
                Path(task.result_path).unlink()
            del task_queue[task_id]
    
    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    temp_dir = Path("/app/outputs/temp")
    if temp_dir.exists():
        for temp_file in temp_dir.glob("*"):
            if temp_file.is_file():
                temp_file.unlink()
    
    return {
        "message": "ç³»ç»Ÿæ¸…ç†å®Œæˆ",
        "memory_after_cleanup": memory_optimizer.get_gpu_memory_info(),
        "remaining_tasks": len(task_queue)
    }

@app.get("/system/stats")
async def get_system_stats():
    """è·å–ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
    memory_info = memory_optimizer.get_gpu_memory_info()
    system_memory = psutil.virtual_memory()
    disk_usage = psutil.disk_usage("/app")
    
    return {
        "timestamp": datetime.now().isoformat(),
        "gpu": memory_info,
        "system_memory": {
            "total": system_memory.total / 1024**3,
            "available": system_memory.available / 1024**3,
            "used": system_memory.used / 1024**3,
            "percentage": system_memory.percent
        },
        "disk": {
            "total": disk_usage.total / 1024**3,
            "used": disk_usage.used / 1024**3,
            "free": disk_usage.free / 1024**3,
            "percentage": (disk_usage.used / disk_usage.total) * 100
        },
        "tasks": {
            "total": len(task_queue),
            "active": len(current_tasks),
            "success_rate": len([t for t in task_queue.values() if t.status == "completed"]) / max(len(task_queue), 1) * 100
        }
    }

if __name__ == "__main__":
    uvicorn.run(
        "api_server_unlimited:app",
        host="0.0.0.0",
        port=8000,
        workers=1,
        log_level="info"
    ) 